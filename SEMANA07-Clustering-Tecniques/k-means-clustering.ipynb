{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad1096a",
   "metadata": {},
   "source": [
    "# K-Means Clustering: Teoría Completa, Métricas y Aplicaciones\n",
    "\n",
    "## 1. Introducción al K-Means Clustering\n",
    "\n",
    "**K-Means** es un **algoritmo de aprendizaje no supervisado** fundamental en el campo del **análisis de clústeres** (o agrupamiento). Su propósito principal es la **partición de un conjunto de datos** en grupos homogéneos, conocidos como **clústeres**. La premisa es simple: dado un conjunto de $N$ observaciones, K-Means las agrupa en $K$ clústeres predefinidos. Cada observación se asigna al clúster cuyo **centroide** (o \"media\") es el más cercano. La meta subyacente es que los puntos dentro de un mismo clúster sean lo más parecidos posible entre sí, y, a su vez, lo más diferentes posible de los puntos en otros clústeres.\n",
    "\n",
    "El nombre \"K-Means\" encapsula su esencia:\n",
    "* **K**: Se refiere al **número preestablecido de clústeres** que deseamos identificar en nuestros datos. Esta es una entrada crucial que el usuario debe proporcionar.\n",
    "* **Means**: Indica que el \"centro\" de cada clúster se calcula como la **media aritmética** de todos los puntos de datos que pertenecen a ese clúster.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Fundamentos Teóricos del Algoritmo\n",
    "\n",
    "El algoritmo K-Means opera mediante un proceso **iterativo**, alternando entre la asignación de puntos de datos a clústeres y la actualización de las posiciones de los centroides hasta que se logra la estabilidad.\n",
    "\n",
    "### 2.1. El Problema de Optimización: Minimización de la Inercia\n",
    "\n",
    "En su núcleo, K-Means es un problema de optimización que busca **minimizar una función de costo** específica, conocida como **inercia** o **Suma de Cuadrados Dentro del Clúster (WCSS - Within-Cluster Sum of Squares)**. La inercia cuantifica la compacidad de los clústeres; es la suma de las distancias al cuadrado de cada punto a su centroide asignado.\n",
    "\n",
    "Formalmente, si tenemos un conjunto de datos $X = \\{x_1, x_2, \\ldots, x_N\\}$, donde cada $x_i \\in \\mathbb{R}^D$ (un punto en un espacio $D$-dimensional), y nuestro objetivo es particionarlos en $K$ clústeres $C = \\{C_1, C_2, \\ldots, C_K\\}$, con centroides respectivos $\\mu = \\{\\mu_1, \\mu_2, \\ldots, \\mu_K\\}$, la función de costo ($J$) a minimizar es:\n",
    "\n",
    "$$J = \\sum_{j=1}^{K} \\sum_{x \\in C_j} \\|x - \\mu_j\\|^2$$\n",
    "\n",
    "Donde:\n",
    "* $J$ es la función de costo (inercia).\n",
    "* $C_j$ representa el conjunto de todos los puntos de datos asignados al clúster $j$.\n",
    "* $\\mu_j$ es el centroide (vector medio) del clúster $j$.\n",
    "* $\\|x - \\mu_j\\|^2$ denota la **distancia euclidiana al cuadrado** entre el punto $x$ y el centroide $\\mu_j$. Minimizar esta suma de cuadrados implica que los puntos dentro de cada clúster deben estar lo más cerca posible de su centroide.\n",
    "\n",
    "### 2.2. Algoritmo Iterativo Paso a Paso\n",
    "\n",
    "El proceso de K-Means se descompone en los siguientes pasos iterativos:\n",
    "\n",
    "1.  **Inicialización de Centroides (Paso de Inicio):**\n",
    "    * Se seleccionan $K$ puntos iniciales para servir como centroides. La elección de estos centroides es crítica, ya que diferentes inicializaciones pueden llevar a diferentes soluciones (mínimos locales).\n",
    "    * Comúnmente, los centroides se eligen **aleatoriamente** del conjunto de datos. Sin embargo, para mejorar la calidad y la robustez del resultado, se prefiere la estrategia **K-Means++**, que selecciona los centroides iniciales de manera más inteligente, intentando que estén bien dispersos.\n",
    "\n",
    "2.  **Asignación de Puntos a Clústeres (Paso de Asignación):**\n",
    "    * Para **cada punto de datos** en el conjunto $X$, se calcula su **distancia** a cada uno de los $K$ centroides existentes.\n",
    "    * Cada punto se asigna al clúster cuyo centroide es el **más cercano** a él. La **distancia euclidiana** es la métrica de distancia más utilizada, aunque otras métricas (como la distancia Manhattan) también pueden emplearse.\n",
    "\n",
    "    $$\\text{Asignación de } x_i \\text{ al clúster } j \\text{ si } \\|x_i - \\mu_j\\|^2 \\leq \\|x_i - \\mu_k\\|^2 \\quad \\forall k \\neq j$$\n",
    "\n",
    "3.  **Actualización de Centroides (Paso de Actualización):**\n",
    "    * Una vez que todos los puntos de datos han sido asignados a un clúster, se **recalcula la posición de cada centroide**.\n",
    "    * El nuevo centroide para cada clúster se establece como el **centro de masa** (la media aritmética) de todos los puntos de datos que fueron asignados a ese clúster en el paso anterior.\n",
    "\n",
    "    $$\\mu_j = \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$$\n",
    "    Donde $|C_j|$ es el número de puntos en el clúster $j$.\n",
    "\n",
    "4.  **Convergencia:**\n",
    "    * Los pasos 2 y 3 se repiten de forma **iterativa** hasta que se cumple una condición de convergencia. Las condiciones típicas incluyen:\n",
    "        * Los centroides ya no cambian su posición significativamente de una iteración a la siguiente.\n",
    "        * La asignación de puntos a los clústeres no varía entre iteraciones.\n",
    "        * Se alcanza un número máximo predefinido de iteraciones.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Métricas y Evaluación de Clústeres\n",
    "\n",
    "Evaluar la calidad de los clústeres generados por K-Means es fundamental para determinar la efectividad del agrupamiento. Las métricas se dividen en dos categorías principales: internas y externas.\n",
    "\n",
    "### 3.1. Métricas Internas (Sin Etiqueta de Verdad)\n",
    "\n",
    "Estas métricas evalúan la calidad del agrupamiento basándose únicamente en la estructura inherente de los datos y los clústeres formados por el algoritmo, sin necesidad de conocer las etiquetas reales de los datos.\n",
    "\n",
    "* **Inercia (Within-Cluster Sum of Squares - WCSS):**\n",
    "    * **Descripción:** Es la misma función de costo que K-Means busca minimizar. Representa la suma de las distancias al cuadrado de cada punto a su centroide asignado. Un valor **más bajo** indica clústeres más compactos y mejor cohesionados.\n",
    "    * **Uso:** Se utiliza principalmente en el **método del \"codo\" (Elbow Method)** para ayudar a determinar el número óptimo de $K$. Se grafica la inercia en función de $K$, y se busca el punto de inflexión (\"codo\") donde la disminución marginal de la inercia se vuelve menos pronunciada.\n",
    "\n",
    "* **Coeficiente de Silueta (Silhouette Score):**\n",
    "    * **Descripción:** Para cada punto de datos, el coeficiente de silueta mide cuán similar es ese punto a su propio clúster (cohesión) en comparación con otros clústeres (separación). Se calcula como:\n",
    "        $$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n",
    "        Donde $a(i)$ es la distancia promedio del punto $i$ a todos los demás puntos en el *mismo* clúster, y $b(i)$ es la distancia promedio del punto $i$ a todos los puntos en el clúster *más cercano* (clúster vecino).\n",
    "    * **Rango:** El coeficiente de silueta varía entre $[-1, 1]$.\n",
    "        * Valores cercanos a **1** indican que el punto está bien agrupado en su propio clúster y bien separado de otros clústeres.\n",
    "        * Valores cercanos a **0** sugieren que el punto está en el límite entre dos clústeres.\n",
    "        * Valores cercanos a **-1** indican que el punto podría haber sido asignado incorrectamente a un clúster.\n",
    "    * **Uso:** Un coeficiente de silueta promedio alto (para todo el conjunto de datos) indica una mejor calidad del agrupamiento. A menudo se utiliza para seleccionar el $K$ óptimo, buscando el valor de $K$ que maximiza el coeficiente de silueta promedio.\n",
    "\n",
    "* **Índice Davies-Bouldin:**\n",
    "    * **Descripción:** Este índice mide la relación entre la dispersión dentro del clúster y la separación entre clústeres. Para cada clúster, calcula la \"similitud\" con el clúster más similar. El índice final es el promedio de estas similitudes máximas.\n",
    "    * **Rango:** Un valor **más bajo** del Índice Davies-Bouldin indica una mejor agrupación, con clústeres más compactos y mejor separados. No tiene un límite superior fijo.\n",
    "    * **Uso:** Al igual que el coeficiente de silueta, se puede utilizar para evaluar y comparar modelos con diferentes valores de $K$, eligiendo el $K$ que minimiza este índice.\n",
    "\n",
    "### 3.2. Métricas Externas (Con Etiqueta de Verdad)\n",
    "\n",
    "Estas métricas son aplicables solo cuando se dispone de las **etiquetas verdaderas** (ground truth) de los datos, permitiendo comparar la partición generada por el algoritmo de clustering con la partición real conocida.\n",
    "\n",
    "* **Puntuación de Homogeneidad (Homogeneity Score):**\n",
    "    * **Descripción:** Un modelo de agrupamiento es homogéneo si cada clúster contiene únicamente miembros de una sola clase (verdadera).\n",
    "    * **Rango:** $[0, 1]$. Un valor de 1 significa homogeneidad perfecta.\n",
    "\n",
    "* **Puntuación de Completitud (Completeness Score):**\n",
    "    * **Descripción:** Un modelo de agrupamiento es completo si todos los miembros de una clase de datos dada se agrupan en el mismo clúster.\n",
    "    * **Rango:** $[0, 1]$. Un valor de 1 significa completitud perfecta.\n",
    "\n",
    "* **V-Measure:**\n",
    "    * **Descripción:** Es la **media armónica** de la homogeneidad y la completitud. Proporciona un equilibrio entre ambas.\n",
    "    * **Rango:** $[0, 1]$. Un valor de 1 indica una agrupación perfecta que es tanto homogénea como completa.\n",
    "\n",
    "* **Índice de Rand Ajustado (Adjusted Rand Index - ARI):**\n",
    "    * **Descripción:** Mide la **similitud entre dos particiones** del mismo conjunto de datos (la partición del clustering y la partición verdadera), ajustada por el azar. Es una medida de \"acuerdo\".\n",
    "    * **Rango:** $[-1, 1]$.\n",
    "        * Un valor de **1** indica una coincidencia perfecta entre las particiones.\n",
    "        * Un valor cercano a **0** indica un acuerdo aleatorio.\n",
    "        * Un valor **negativo** indica un acuerdo peor que el aleatorio.\n",
    "    * **Ventaja:** Es una métrica robusta que no se ve afectada por el número de clústeres o el desequilibrio de clases.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Cuándo Utilizar K-Means Clustering\n",
    "\n",
    "K-Means es un algoritmo potente y ampliamente utilizado, pero su efectividad depende de las características de los datos y de los objetivos del análisis. Es particularmente útil cuando:\n",
    "\n",
    "* **El número de clústeres ($K$) es conocido o se puede estimar razonablemente:** K-Means exige que $K$ sea especificado de antemano. Si $K$ es desconocido, las métricas internas como el método del codo o el coeficiente de silueta pueden ayudar a su estimación.\n",
    "* **Los clústeres son de forma esférica o globular y de tamaño similar:** K-Means asume implícitamente que los clústeres son convexos y tienen una varianza similar en todas las direcciones. Si los clústeres tienen formas irregulares (como formas de \"C\", anulares o entrelazadas) o densidades muy diferentes, K-Means puede no ofrecer los mejores resultados.\n",
    "* **El conjunto de datos es grande:** K-Means es computacionalmente eficiente y escala bien a grandes volúmenes de datos. Su complejidad temporal es $O(N \\cdot K \\cdot I \\cdot D)$, donde $N$ es el número de puntos, $K$ es el número de clústeres, $I$ es el número de iteraciones y $D$ es la dimensionalidad de los datos.\n",
    "* **Se desea una agrupación simple y rápida:** Dada su simplicidad conceptual, K-Means es fácil de implementar y entender, siendo un excelente punto de partida para muchos problemas de clustering.\n",
    "\n",
    "### Casos de Uso Comunes:\n",
    "\n",
    "* **Segmentación de clientes:** Agrupar clientes con base en patrones de compra, datos demográficos o comportamientos de navegación para campañas de marketing personalizadas.\n",
    "* **Análisis de imágenes:** Cuantificación de color, compresión de imágenes o detección de objetos mediante la agrupación de píxeles.\n",
    "* **Agrupamiento de documentos:** Organizar grandes colecciones de documentos por temas o categorías para facilitar la búsqueda y el análisis.\n",
    "* **Detección de anomalías:** Identificar puntos de datos que no se ajustan bien a ningún clúster, lo que podría indicar comportamientos inusuales o fraudes.\n",
    "* **Bioinformática:** Agrupación de secuencias de ADN, perfiles de expresión génica o análisis de proteínas.\n",
    "* **Sistemas de recomendación:** Agrupar usuarios o ítems con gustos o características similares para generar recomendaciones personalizadas.\n",
    "\n",
    "### Cuándo NO Utilizar K-Means:\n",
    "\n",
    "* **Clústeres con formas arbitrarias o no convexas:** K-Means puede tener dificultades significativas para identificar clústeres que no son inherentemente esféricos. Algoritmos como DBSCAN o Gaussian Mixture Models (GMM) pueden ser más adecuados.\n",
    "* **Clústeres con densidades muy diferentes:** Si algunos clústeres son mucho más densos que otros, K-Means puede dividir erróneamente clústeres densos o fusionar clústeres dispersos.\n",
    "* **Presencia significativa de ruido o valores atípicos:** K-Means es sensible a los valores atípicos, ya que estos pueden \"tirar\" de los centroides, distorsionando la forma y posición de los clústeres y afectando negativamente la calidad del agrupamiento.\n",
    "* **Cuando se desconoce por completo el número óptimo de clústeres y no hay una forma clara de estimarlo:** Si bien existen métodos de estimación, en escenarios de incertidumbre total, algoritmos que determinan $K$ de forma automática (como DBSCAN) podrían ser preferibles.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. K-Means++: Una Mejor Inicialización\n",
    "\n",
    "La estrategia de **inicialización aleatoria** de centroides en K-Means, aunque simple, puede llevar a soluciones subóptimas y a la convergencia en mínimos locales. Para mitigar este problema, se desarrolló **K-Means++**, un algoritmo de inicialización más inteligente que busca seleccionar los centroides iniciales de manera que estén **bien dispersos** a lo largo del conjunto de datos.\n",
    "\n",
    "### Pasos de K-Means++:\n",
    "\n",
    "1.  **Primer Centroide:** Se elige un punto de datos aleatoriamente del conjunto como el primer centroide.\n",
    "2.  **Selección de Centroides Subsecuentes:** Para cada punto de datos restante $x$, se calcula la distancia al cuadrado $D(x)^2$ al centroide más cercano que ya ha sido elegido.\n",
    "3.  **Probabilidad Ponderada:** Se selecciona un nuevo punto de datos para que sea el siguiente centroide con una probabilidad proporcional a $D(x)^2$. Esto significa que los puntos que están más lejos de los centroides existentes tienen una mayor probabilidad de ser seleccionados, lo que fomenta una distribución más uniforme de los centroides iniciales.\n",
    "4.  **Repetición:** Los pasos 2 y 3 se repiten hasta que se hayan seleccionado un total de $K$ centroides.\n",
    "\n",
    "Esta estrategia de inicialización, aunque añade un poco de complejidad computacional al inicio, a menudo conduce a una **convergencia más rápida** y a la obtención de **mejores resultados finales** (es decir, una inercia globalmente más baja) al evitar muchos mínimos locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1d9c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
